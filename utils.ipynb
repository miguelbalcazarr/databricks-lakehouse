{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "074ed543-5521-4fc5-beca-d8ee4fdd9dbe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Librerias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "823ff7d0-69f6-4c86-bbc4-16e2c3f3ffb3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import DataFrame\n",
    "from typing import Tuple, Dict\n",
    "from pyspark.sql.functions import col, current_timestamp, lit, when\n",
    "from pyspark.sql.types import StringType\n",
    "from delta.tables import DeltaTable\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4223206d-1f5b-4a4d-877c-884048c41ab1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cc20f43e-bcce-4fd5-a07f-ff5cfa051a65",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def variables_globales() -> Dict:\n",
    "    \"\"\"\n",
    "    Recupera y devuelve las variables globales de configuración de almacenamiento.\n",
    "\n",
    "    Returns:\n",
    "        Dict: Un diccionario con:\n",
    "            - container (str): Nombre del contenedor en ADLS Gen2,\n",
    "              obtenido desde el Secret Scope \"scope-mbc\" y la clave \"secret-env-container\".\n",
    "            - storage_account (str): Nombre de la cuenta de almacenamiento,\n",
    "              obtenido desde el Secret Scope \"scope-mbc\" y la clave \"secret-env-storage-account\".\n",
    "            - path_base (str): Ruta base ABFSS al contenedor, en el formato:\n",
    "              \"abfss://<container>@<storage_account>.dfs.core.windows.net\"\n",
    "    \"\"\"\n",
    "    # Recupera el nombre del contenedor desde Key Vault\n",
    "    container = dbutils.secrets.get(\"scope-mbc\", \"secret-env-container\")\n",
    "    # Recupera el nombre de la cuenta de almacenamiento desde Key Vault\n",
    "    storage_account = dbutils.secrets.get(\"scope-mbc\", \"secret-env-storage-account\")\n",
    "    # Construye la ruta base ABFSS al contenedor montado en ADLS Gen2\n",
    "    path_base = f\"abfss://{container}@{storage_account}.dfs.core.windows.net\"\n",
    "    \n",
    "    return {\n",
    "        \"container\": container,\n",
    "        \"storage_account\": storage_account,\n",
    "        \"path_base\": path_base\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1c96a2d4-bc9e-4e2b-9c00-34998fd0e8a1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Funciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "61586622-4a7e-4609-a7ce-76e2bd9f69f6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def read_landing(\n",
    "    path: str\n",
    ") -> DataFrame:\n",
    "    \"\"\"\n",
    "    Lee datos crudos desde la carpeta de Landing en formato Parquet y\n",
    "    normaliza todas las columnas a tipo string para mantener uniformidad.\n",
    "\n",
    "    Args:\n",
    "        path (str): Ruta relativa dentro de la carpeta Landing, por ejemplo:\n",
    "                    \"bronze/sales/CountryRegionCurrency\".\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: Spark DataFrame con todas las columnas casteadas a string,\n",
    "                   listo para procesar en la capa Bronze o Silver.\n",
    "    \"\"\"\n",
    "    # Obtiene la ruta base de ADLS Gen2 (abfss://container@storage_account.dfs.core.windows.net)\n",
    "    path_base = variables_globales()[\"path_base\"]\n",
    "\n",
    "    # Carga los datos crudos en Parquet desde la ubicación completa\n",
    "    df = spark.read.format(\"parquet\").load(f\"{path_base}/{path}\")\n",
    "\n",
    "    # Para evitar inconsistencias de tipos en downstream, convierte todas las columnas a string\n",
    "    columns_to_cast = [\n",
    "        col(c).cast(\"string\").alias(c)  # Mantiene el mismo nombre de columna\n",
    "        for c in df.columns\n",
    "    ]\n",
    "\n",
    "    # Devuelve el DataFrame con el casting aplicado en todas las columnas\n",
    "    return df.select(*columns_to_cast)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a24c730e-8bc4-4c2c-a537-0cefd539a130",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def write_bronze(\n",
    "    df: DataFrame,\n",
    "    table_name: str\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Escribe datos en la tabla Bronze (Delta) usando un merge para solo insertar\n",
    "    los registros nuevos, sin actualizar los existentes.\n",
    "\n",
    "    Args:\n",
    "        df (DataFrame): DataFrame de Spark con los datos crudos a insertar.\n",
    "        table_name (str): Nombre completo de la tabla Delta en el metastore\n",
    "                          (por ejemplo \"bronze.sales_countryregioncurrency\").\n",
    "\n",
    "    Returns:\n",
    "        None: La función imprime un log de la operación y llama a `insert_metrics()`\n",
    "              para recopilar y almacenar métricas de ingesta.\n",
    "    \"\"\"\n",
    "\n",
    "    # Carga la tabla Delta existente por nombre\n",
    "    delta_table = DeltaTable.forName(spark, table_name)\n",
    "\n",
    "    # Prepara el mapeo de columnas para el insert:\n",
    "    # todas las columnas de la tabla excepto la columna de auditoría IngestionDate\n",
    "    columns_to_insert = {\n",
    "        col_name: f\"in.{col_name}\"\n",
    "        for col_name in delta_table.toDF().columns\n",
    "        if col_name not in [\"IngestionDate\"]\n",
    "    }\n",
    "\n",
    "    # Ejecuta el MERGE:\n",
    "    # - Condición de match: lit(False) para que nunca haya coincidencia,\n",
    "    #   de modo que todos los registros vengan por la rama whenNotMatched\n",
    "    # - whenNotMatchedInsert: inserta todos los registros del DataFrame df\n",
    "    delta_table.alias(\"m\") \\\n",
    "        .merge(\n",
    "            df.alias(\"in\"),\n",
    "            lit(False)  # fuerza siempre el insert-only\n",
    "        ) \\\n",
    "        .whenNotMatchedInsert(values=columns_to_insert) \\\n",
    "        .execute()\n",
    "\n",
    "    # Log de confirmación\n",
    "    print(f\"Insert ejecutado en la tabla {table_name}\")\n",
    "\n",
    "    # Llama a la función de métricas tras la inserción\n",
    "    return insert_metrics(get_metrics(table_name))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ccebe63a-fdc1-47c3-b039-0cb810ad3665",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def get_metrics(\n",
    "    table_name: str\n",
    ") -> Tuple[Dict[str, int], str]:\n",
    "    \"\"\"\n",
    "    Extrae métricas de la última operación Delta Lake (MERGE/WRITE) realizada sobre una tabla Delta.\n",
    "\n",
    "    Args:\n",
    "        table_name (str): Nombre completo de la tabla Delta en el metastore\n",
    "                          (p. ej. \"bronze.sales_countryregioncurrency\").\n",
    "\n",
    "    Returns:\n",
    "        Tuple[Dict[str, int], str]:\n",
    "            - Dict[str, int]: Diccionario con las métricas de operación extraídas de `operationMetrics`,\n",
    "              convertido a enteros. Posibles claves:\n",
    "                * numInsertedRows   — número de filas insertadas\n",
    "                * numUpdatedRows    — número de filas actualizadas\n",
    "                * numDeletedRows    — número de filas eliminadas\n",
    "                * numOutputBytes    — bytes escritos en disco\n",
    "            - str: El mismo `table_name`, para facilitar su paso a funciones encadenadas.\n",
    "    \"\"\"\n",
    "\n",
    "    # Carga la tabla Delta por su nombre en el metastore\n",
    "    delta_table = DeltaTable.forName(spark, table_name)\n",
    "\n",
    "    try:\n",
    "        # Recupera el historial de transacciones, ordenado de más reciente a más antiguo\n",
    "        history_df = delta_table.history()\n",
    "\n",
    "        # Recorre cada fila del historial buscando la operación MERGE o WRITE más reciente\n",
    "        for row in history_df.collect():\n",
    "            if row.operation in (\"MERGE\", \"WRITE\"):\n",
    "                # Obtiene el diccionario de métricas de la operación\n",
    "                raw_metrics = row.asDict().get(\"operationMetrics\", {})\n",
    "                # Convierte cada valor numérico de string a int, ignorando claves no numéricas\n",
    "                numeric_metrics = {}\n",
    "                for key, value in raw_metrics.items():\n",
    "                    try:\n",
    "                        numeric_metrics[key] = int(value)\n",
    "                    except (ValueError, TypeError):\n",
    "                        # Si no es un número, se omite\n",
    "                        continue\n",
    "                return numeric_metrics, table_name\n",
    "\n",
    "        # Si no encuentra operación MERGE/WRITE, devuelve diccionario vacío\n",
    "        return {}, table_name\n",
    "\n",
    "    except Exception as e:\n",
    "        # En caso de error al obtener el historial, imprime el error y devuelve vacío\n",
    "        print(f\"Warning: no se pudieron extraer métricas de {table_name}: {e}\")\n",
    "        return {}, table_name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "16eeb0c5-fde9-4e36-ba46-35b6c1d7fb5c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def merge(\n",
    "    table_name: str,\n",
    "    df: DataFrame,\n",
    "    identity_column: list = [],\n",
    "    enable_delete: bool = False\n",
    ") -> Dict:\n",
    "    \"\"\"\n",
    "    Ejecuta un MERGE dinámico sobre una tabla Delta Lake usando su clave primaria.\n",
    "\n",
    "    Esta función:\n",
    "      1. Consulta automáticamente las columnas de clave primaria desde el metastore Delta.\n",
    "      2. Construye las condiciones de merge basadas en esas columnas.\n",
    "      3. Excluye columnas de auditoría o identidad de las operaciones de actualización/inserción.\n",
    "      4. Inserta nuevas filas y actualiza existentes.\n",
    "      5. Opcionalmente, elimina filas en destino que ya no estén en el DataFrame fuente.\n",
    "      6. Devuelve las métricas (insertadas, actualizadas, eliminadas) para alimentar la tabla de control.\n",
    "\n",
    "    Args:\n",
    "        table_name (str): Nombre de la tabla destino en formato 'schema.table'.\n",
    "        df (DataFrame): DataFrame fuente con los registros a mergear.\n",
    "        identity_column (list, optional): Lista de columnas de identidad (surrogate keys)\n",
    "                                          que no deben insertarse manualmente. Default: [].\n",
    "        enable_delete (bool, optional): Si True, elimina en destino las filas ausentes\n",
    "                                        en el DataFrame fuente. Default: False.\n",
    "\n",
    "    Returns:\n",
    "        Dict: Métricas extraídas tras ejecutar el merge, con las claves:\n",
    "              - numInsertedRows\n",
    "              - numUpdatedRows\n",
    "              - numDeletedRows\n",
    "              - numOutputBytes\n",
    "    \"\"\"\n",
    "\n",
    "    # 1. Recuperar las columnas que componen la clave primaria de la tabla\n",
    "    query = f\"\"\"\n",
    "    SELECT cu.column_name\n",
    "      FROM system.information_schema.key_column_usage AS cu\n",
    "      JOIN system.information_schema.table_constraints AS tc\n",
    "        ON cu.constraint_catalog = tc.constraint_catalog\n",
    "       AND cu.constraint_schema  = tc.constraint_schema\n",
    "       AND cu.constraint_name    = tc.constraint_name\n",
    "     WHERE concat_ws('.', cu.table_schema, cu.table_name) = '{table_name}'\n",
    "       AND tc.constraint_type = 'PRIMARY KEY'\n",
    "       AND cu.table_catalog = 'lakehouse'\n",
    "     ORDER BY cu.ordinal_position\n",
    "    \"\"\"\n",
    "    df_query = spark.sql(query)\n",
    "    columns_key = [row['column_name'] for row in df_query.collect()]\n",
    "\n",
    "    # 2. Construir la condición de merge usando cada columna de la clave primaria\n",
    "    merge_conditions = \" AND \".join([f\"m.{c} = src.{c}\" for c in columns_key])\n",
    "\n",
    "    # 3. Cargar la tabla Delta destino\n",
    "    delta_table = DeltaTable.forName(spark, table_name)\n",
    "    target_columns = delta_table.toDF().columns\n",
    "\n",
    "    # 4. Definir qué columnas actualizar (excluyendo PKs y columnas de auditoría)\n",
    "    exclusion_list_update = set(columns_key + [\"FechaAuditoriaCreacion\"])\n",
    "    columns_to_update = {\n",
    "        col: f\"src.{col}\"\n",
    "        for col in target_columns\n",
    "        if col not in exclusion_list_update\n",
    "    }\n",
    "\n",
    "    # 5. Definir qué columnas insertar (excluyendo columnas de identidad)\n",
    "    exclusion_list_insert = set(identity_column)\n",
    "    columns_to_insert = {\n",
    "        col: f\"src.{col}\"\n",
    "        for col in target_columns\n",
    "        if col not in exclusion_list_insert\n",
    "    }\n",
    "\n",
    "    # 6. Construir el builder de MERGE\n",
    "    merge_builder = (\n",
    "        delta_table.alias(\"m\")\n",
    "            .merge(df.alias(\"src\"), merge_conditions)\n",
    "            .whenMatchedUpdate(set=columns_to_update)\n",
    "            .whenNotMatchedInsert(values=columns_to_insert)\n",
    "    )\n",
    "\n",
    "    # 7. Si se habilita delete, eliminar filas en destino no presentes en la fuente\n",
    "    if enable_delete:\n",
    "        merge_builder = merge_builder.whenNotMatchedBySourceDelete()\n",
    "\n",
    "    # 8. Ejecutar el MERGE\n",
    "    merge_builder.execute()\n",
    "    print(f\"Merge ejecutado en la tabla {table_name}\")\n",
    "\n",
    "    # 9. Registrar y devolver métricas para la tabla de control\n",
    "    return insert_metrics(get_metrics(table_name))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "334245ce-fcfd-41fd-863c-97c5987bd768",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def insert_metrics(metrics_tuple: Tuple[Dict[str, int], str]) -> None:\n",
    "    \"\"\"\n",
    "    Inserta las métricas de la última operación de MERGE/WRITE en la tabla de auditoría Delta.\n",
    "\n",
    "    Args:\n",
    "        metrics_tuple (Tuple[Dict[str, int], str]):\n",
    "            - metrics (Dict[str, int]): Diccionario con métricas extraídas de Delta:\n",
    "                * numTargetRowsInserted   — filas insertadas\n",
    "                * numTargetRowsUpdated    — filas actualizadas\n",
    "                * numTargetRowsDeleted    — filas eliminadas\n",
    "                * numTargetBytesAdded     — bytes escritos en disco\n",
    "                * executionTimeMs         — tiempo de ejecución del merge en milisegundos\n",
    "            - table_name (str): Nombre completo de la tabla destino en formato 'layer.table'.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: Si el nombre de la tabla no contiene un punto separador.\n",
    "    \"\"\"\n",
    "\n",
    "    # Desempaqueta la tupla de métricas\n",
    "    metrics, table_name = metrics_tuple\n",
    "\n",
    "    # Recupera identificadores de ejecución desde widgets de Databricks\n",
    "    job_id       = int(dbutils.widgets.get('JobId'))\n",
    "    job_run_id   = int(dbutils.widgets.get('JobRunId'))\n",
    "    task_run_id  = int(dbutils.widgets.get('TaskRunId'))\n",
    "    start_time   = dbutils.widgets.get('StartTime')  # como string\n",
    "\n",
    "    # Valida formato de table_name y separa layer y nombre de tabla\n",
    "    if '.' not in table_name:\n",
    "        raise ValueError(f\"El nombre de tabla '{table_name}' debe estar en formato 'layer.table'\")\n",
    "    layer, table = table_name.split(\".\")\n",
    "\n",
    "    # Construye un DataFrame con las métricas y añade columnas de contexto\n",
    "    df_metrics = (\n",
    "        spark.createDataFrame([metrics])\n",
    "            # Contexto de Job/Task\n",
    "            .withColumn('job_id', lit(job_id))\n",
    "            .withColumn('job_run_id', lit(job_run_id))\n",
    "            .withColumn('task_run_id', lit(task_run_id))\n",
    "            .withColumn('job_start_time', lit(start_time).cast(\"timestamp\"))\n",
    "            .withColumn('job_end_time', current_timestamp())\n",
    "            .withColumn('job_duration_seconds',\n",
    "                        (col(\"job_end_time\").cast(\"long\") - col(\"job_start_time\").cast(\"long\")))\n",
    "            # Contexto de tabla y capa\n",
    "            .withColumn('table', lit(table))\n",
    "            .withColumn('layer', lit(layer))\n",
    "            # Métricas de ingesta\n",
    "            .withColumn('rows_in',\n",
    "                        when(col(\"layer\") != \"bronze\", lit(0))\n",
    "                        .otherwise(col('numTargetRowsInserted')))\n",
    "            .withColumn('rows_inserted', col(\"numTargetRowsInserted\"))\n",
    "            .withColumn('rows_updated', col(\"numTargetRowsUpdated\"))\n",
    "            .withColumn('rows_deleted', col(\"numTargetRowsDeleted\"))\n",
    "            .withColumn('file_bytes', col(\"numTargetBytesAdded\"))\n",
    "            .withColumn('merge_duration_seconds',\n",
    "                        col(\"executionTimeMs\") / lit(1000))\n",
    "            # Estado de la operación\n",
    "            .withColumn('job_status',\n",
    "                        when(col(\"file_bytes\") > 0, lit(\"Success\"))\n",
    "                        .otherwise(lit(\"Failed\")))\n",
    "            # Selecciona el orden de columnas definitivo para la tabla de auditoría\n",
    "            .select(\n",
    "                \"job_id\", \"job_run_id\", \"task_run_id\",\n",
    "                \"job_start_time\", \"job_end_time\", \"job_duration_seconds\",\n",
    "                \"job_status\", \"table\", \"layer\",\n",
    "                \"rows_in\", \"rows_inserted\", \"rows_updated\", \"rows_deleted\",\n",
    "                \"file_bytes\", \"merge_duration_seconds\"\n",
    "            )\n",
    "    )\n",
    "\n",
    "    # Carga la tabla Delta de auditoría\n",
    "    delta_table = DeltaTable.forName(spark, \"auditoria.ingestion_log\")\n",
    "\n",
    "    # Prepara el mapeo de columnas para inserción, excluyendo snapshot_time\n",
    "    columns_to_insert = {\n",
    "        col_name: f\"in.{col_name}\"\n",
    "        for col_name in delta_table.toDF().columns\n",
    "        if col_name not in [\"snapshot_time\"]\n",
    "    }\n",
    "\n",
    "    # Ejecuta MERGE forzando solo inserts (lit(False) evita coincidencias)\n",
    "    delta_table.alias(\"m\") \\\n",
    "        .merge(\n",
    "            df_metrics.alias(\"in\"),\n",
    "            lit(False)\n",
    "        ) \\\n",
    "        .whenNotMatchedInsert(values=columns_to_insert) \\\n",
    "        .execute()\n",
    "\n",
    "    # Log de confirmación\n",
    "    print(\"Métricas insertadas en auditoria.ingestion_log\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 7205989130611933,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "utils",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
