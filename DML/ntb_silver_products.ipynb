{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "72cbc76b-3a73-4a7e-8d3e-ca5c3666c1f8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# def clean_order_items_data(df):\n",
    "#     \"\"\"Transformaciones específicas para ítems de órdenes\"\"\"\n",
    "#     from pyspark.sql.functions import col, when, round, current_timestamp\n",
    "    \n",
    "#     return df \\\n",
    "#         # Validación y limpieza de cantidad\n",
    "#         .withColumn(\"quantity_clean\",\n",
    "#                    when(col(\"quantity\").isNull() | (col(\"quantity\") <= 0), 1)\n",
    "#                    .otherwise(col(\"quantity\"))) \\\n",
    "        \n",
    "#         # Validación de precios\n",
    "#         .withColumn(\"list_price_clean\",\n",
    "#                    when(col(\"list_price\").isNull() | (col(\"list_price\") <= 0), 0.01)\n",
    "#                    .otherwise(round(col(\"list_price\"), 2))) \\\n",
    "        \n",
    "#         # Validación de descuentos\n",
    "#         .withColumn(\"discount_clean\",\n",
    "#                    when(col(\"discount\").isNull(), 0.0)\n",
    "#                    .when(col(\"discount\") < 0, 0.0)\n",
    "#                    .when(col(\"discount\") > 0.99, 0.99)  # Máximo 99% de descuento\n",
    "#                    .otherwise(round(col(\"discount\"), 2))) \\\n",
    "        \n",
    "#         # Cálculo de valores derivados\n",
    "#         .withColumn(\"net_price\", \n",
    "#                    round(col(\"list_price_clean\") * (1 - col(\"discount_clean\")), 2)) \\\n",
    "#         .withColumn(\"line_total\", \n",
    "#                    round(col(\"net_price\") * col(\"quantity_clean\"), 2)) \\\n",
    "        \n",
    "#         # Validación de relaciones con otras tablas\n",
    "#         .withColumn(\"valid_product\",\n",
    "#                    when(col(\"product_id\").isin(get_valid_product_ids()), 1)\n",
    "#                    .otherwise(0)) \\\n",
    "        \n",
    "#         # Metadata\n",
    "#         .withColumn(\"processed_at\", current_timestamp()) \\\n",
    "#         .withColumn(\"processing_date\", current_date()) \\\n",
    "        \n",
    "#         # Filtrar registros inválidos\n",
    "#         .filter(col(\"valid_product\") == 1) \\\n",
    "#         .drop(\"valid_product\")\n",
    "\n",
    "# def clean_orders_data(df):\n",
    "#     \"\"\"Transformaciones específicas para órdenes\"\"\"\n",
    "#     from pyspark.sql.functions import col, when, datediff, current_date, to_date, current_timestamp\n",
    "    \n",
    "#     return df \\\n",
    "#         .withColumn(\"order_date\", to_date(col(\"order_date\"))) \\\n",
    "#         .withColumn(\"required_date\", to_date(col(\"required_date\"))) \\\n",
    "#         .withColumn(\"shipped_date\", \n",
    "#                    when((col(\"shipped_date\").isNotNull()) & \n",
    "#                         (col(\"shipped_date\") >= col(\"order_date\")) & \n",
    "#                         (col(\"shipped_date\") <= current_date()),\n",
    "#                    to_date(col(\"shipped_date\")))) \\\n",
    "#         .withColumn(\"order_status_clean\",\n",
    "#                    when(~col(\"order_status\").isin([1, 2, 3, 4]), 1)\n",
    "#                    .when((col(\"shipped_date\").isNull()) & (col(\"order_status\") == 4), 2)\n",
    "#                    .otherwise(col(\"order_status\"))) \\\n",
    "#         .withColumn(\"days_to_ship\",\n",
    "#                    when(col(\"shipped_date\").isNotNull(),\n",
    "#                         datediff(col(\"shipped_date\"), col(\"order_date\")))) \\\n",
    "#         .withColumn(\"processed_at\", current_timestamp())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "df1a3e93-edf6-4614-bbe3-b22de60a6188",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, when, round, trim, initcap, year, current_date, current_timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ec77e58d-df02-43f8-8ff3-ad1d12418428",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "container = dbutils.secrets.get(\"scope-mbc\", \"secret-env-container\")\n",
    "storage_account = dbutils.secrets.get(\"scope-mbc\", \"secret-env-storage-account\")\n",
    "path_base = f\"abfss://{container}@{storage_account}.dfs.core.windows.net\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1593ce28-3c6b-4612-86bb-eec94a85e0dc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Configuración de nombres de tablas (managed)\n",
    "tabla_bronze = \"bronze.products\"  # Ejemplo con tabla de órdenes en Bronze\n",
    "tabla_silver = \"silver.products\"  # Tabla destino en Silver\n",
    "path_checkpoint = f\"{path_base}/checkpoints/bronze_to_silver/{tabla_silver}/\"\n",
    "\n",
    "# Limpieza de checkpoint (usar solo cuando sea necesario)\n",
    "# dbutils.fs.rm(path_checkpoint, recurse=True)\n",
    "\n",
    "def transform_to_silver(batch_df, batch_id):\n",
    "    \"\"\"\n",
    "    Función de transformación para procesamiento por lotes\n",
    "    \"\"\"\n",
    "    # Ejemplo de transformación para la tabla de órdenes\n",
    "    if tabla_silver.endswith(\"orders\"):\n",
    "        transformed_df = batch_df.transform(clean_orders_data)\n",
    "    elif tabla_silver.endswith(\"products\"):\n",
    "        transformed_df = batch_df.transform(clean_products_data)\n",
    "    elif tabla_silver.endswith(\"order_items\"):\n",
    "        transformed_df = batch_df.transform(clean_order_items_data)\n",
    "    else:\n",
    "        transformed_df = batch_df  # Transformación por defecto\n",
    "    \n",
    "    # Escribir el batch transformado a tabla managed\n",
    "    transformed_df.write \\\n",
    "        .format(\"delta\") \\\n",
    "        .mode(\"append\") \\\n",
    "        .saveAsTable(tabla_silver)  # Esto crea/actualiza una tabla managed\n",
    "\n",
    "def clean_products_data(df):\n",
    "    \"\"\"Transformaciones específicas para productos\"\"\"\n",
    "    \n",
    "    current_year = year(current_date())\n",
    "    \n",
    "    return df \\\n",
    "        .withColumn(\"product_name\", trim(col(\"product_name\"))) \\\n",
    "        .withColumn(\"brand_name\", initcap(trim(col(\"brand_name\")))) \\\n",
    "        .withColumn(\"category_name\", initcap(trim(col(\"category_name\")))) \\\n",
    "        .withColumn(\"model_year_clean\",\n",
    "                   when(col(\"model_year\") < 2000, 2000)\n",
    "                   .when(col(\"model_year\") > current_year + 1, current_year)\n",
    "                   .otherwise(col(\"model_year\"))) \\\n",
    "        .withColumn(\"list_price_clean\",\n",
    "                   when(col(\"list_price\") <= 0, 1.00)\n",
    "                   .otherwise(round(col(\"list_price\"), 2))) \\\n",
    "        .withColumn(\"processed_at\", current_timestamp())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1c6dd6d6-37ca-4d7a-aae1-4081da83e6a6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Iniciar el stream leyendo desde tabla managed\n",
    "df_stream = (\n",
    "    spark.readStream\n",
    "    .format(\"delta\")\n",
    "    .table(tabla_bronze)  # Leer desde tabla managed en lugar de path\n",
    "    .writeStream\n",
    "    .foreachBatch(transform_to_silver)  # Aplicar transformaciones por batch\n",
    "    .option(\"checkpointLocation\", path_checkpoint)\n",
    "    #.trigger(availableNow=True)  # Para procesamiento en lotes\n",
    "    # .trigger(processingTime='1 minute')  # Para procesamiento continuo\n",
    "    .start()\n",
    ")\n",
    "\n",
    "# Esperar a que termine el procesamiento\n",
    "df_stream.awaitTermination()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "ntb_silver_products",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
